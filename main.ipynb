{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "conv2.uc2018",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlirezaKhodabakhsh/Genetic_EMG/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep network optimization using a Genetic algorithm for recognizing hand gestures from EMG Signal\n",
    "- **Kasra Sehhat** [[Email](kasra.sehat@sharif.edu)] [[LinkedIn](linkedin.com/in/kasra-sehat)][[GitHub](https://github.com/kasrasehat)]\n",
    "- **Alireza Khodabakhsh** [[Email](alireza.khodabakhsh@ee.sharif.edu)][[LinkedIn](https://www.linkedin.com/in/alirezakhodabakhsh/)][[GitHub](https://github.com/AlirezaKhodabakhsh)]\n",
    "- **Arman Aghaee** [[Email](arman.aghaee@ryerson.ca)] [[LinkedIn](https://www.linkedin.com/in/armanaghaee/)][[GitHub](https://github.com/armanaghaee)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0Vl1aTCiJlvN"
   },
   "source": [
    "'''\n",
    "In this section all required libraries are imported\n",
    "'''\n",
    "#!pip install keras.optimizers\n",
    "import keras.optimizers\n",
    "a = keras.optimizers.adam_v2\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "import csv\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import sys"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "This section is downloading uc2018 dataset which contains EMG signals related to 8 hand gestures\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "link = 'https://zenodo.org/record/1320922/files/dualmyo_dataset.pkl?download=1'\n",
    "file_name = \"dualmyo_dataset.pkl\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "    print(\"Downloading %s\" % file_name)\n",
    "    response = requests.get(link, stream=True)\n",
    "    total_length = response.headers.get('content-length')\n",
    "\n",
    "    if total_length is None: # no content length header\n",
    "        f.write(response.content)\n",
    "    else:\n",
    "        dl = 0\n",
    "        total_length = int(total_length)\n",
    "        for data in response.iter_content(chunk_size=4096):\n",
    "            dl += len(data)\n",
    "            f.write(data)\n",
    "            done = int(50 * dl / total_length)\n",
    "            sys.stdout.write(\"\\r[%s%s]\" % ('=' * done, ' ' * (50-done)) )\n",
    "            sys.stdout.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LLORpFz-4n6T",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "outputId": "d449ef5b-0a8d-4996-fc51-a5aeb57f409b"
   },
   "source": [
    "\"\"\"\n",
    "def mode_rows(a):\n",
    "    a = np.ascontiguousarray(a)\n",
    "    void_dt = np.dtype((np.void, a.dtype.itemsize * np.prod(a.shape[1:])))\n",
    "    _,ids, count = np.unique(a.view(void_dt).ravel(), \\\n",
    "                                return_index=1,return_counts=1)\n",
    "    largest_count_id = ids[count.argmax()]\n",
    "    most_frequent_row = a[largest_count_id]\n",
    "    return most_frequent_row\n",
    "\n",
    "a = pd.read_pickle(r'/content/drive/My Drive/dualmyo_dataset.pkl')\n",
    "signal_list =a[0]\n",
    "label_list =a[1]\n",
    "\n",
    "signal = np.array([0,0,0,0,0,0,0,0])\n",
    "k = 0\n",
    "coef = np.ones([400,1])\n",
    "target = np.array([0])\n",
    "\n",
    "for s in signal_list :\n",
    "    signal = np.vstack((signal,abs(s[:,2:10])))\n",
    "    signal = np.vstack((signal,abs(s[:,12:20])))\n",
    "    target  = np.vstack((target,label_list[k]*coef))\n",
    "    target  = np.vstack((target,label_list[k]*coef))\n",
    "    k = k + 1\n",
    "\n",
    "print(signal.shape)    \n",
    "print(target.shape)   \n",
    "\n",
    "data = np.concatenate((signal,target),axis=1) \n",
    "\n",
    "seq_len = 200\n",
    "stride = 200\n",
    "dataset=[]\n",
    "\n",
    "for i in range(0 , len(data)-seq_len , stride):\n",
    "    sample =  data[i:i+seq_len,:]\n",
    "    dataset.append(sample)\n",
    "    \n",
    "np.random.shuffle(dataset)        \n",
    "train, test = sklearn.model_selection.train_test_split(dataset , train_size =int( np.round(0.85*len(dataset))) , test_size = int(np.round(0.15*len(dataset))))\n",
    "\n",
    "\n",
    "x_train = np.zeros([len(train),seq_len,8,1])\n",
    "y_train = np.zeros([len(train),1])\n",
    "f=0\n",
    "for s  in train :\n",
    "    x_train[f,:,:,:] = s[:,0:8].reshape(-1,8,1)\n",
    "    y_train[f, 0] = mode_rows(s[seq_len-20:seq_len,8])\n",
    "    f = f+1\n",
    "\n",
    "x_tra = (x_train/128).astype('float32')\n",
    "y_tra = np_utils.to_categorical(y_train) \n",
    "\n",
    "\n",
    "\n",
    "x_test = np.zeros([len(test),seq_len,8,1])\n",
    "y_test = np.zeros([len(test),1])\n",
    "p=0\n",
    "for s  in test :\n",
    "    x_test[p,:,:,:] = s[:,0:8].reshape(-1,8,1)\n",
    "    y_test[p, 0] = mode_rows(s[seq_len-20:seq_len,8])\n",
    "    p = p+1\n",
    "\n",
    "x_tes = (x_test/128).astype('float32')\n",
    "\n",
    "y_tes = np_utils.to_categorical(y_test) \n",
    "\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "In uc2018 data set each subject has 2 myoarmband which recorded the emg signal. Therefor data structured from 16 channels and N timesteps. In other viewpoint each myoarmband for each subject could be considered as independent subject. In such condition there would be data with structure of 8 channels and 2N timesteps.\n",
    "Preprocessing including 4 steps:\n",
    "1_ Normalization: Because the minimum and maximum of EMG signal is known, so the best method for normalization is minmax method. After normalization all data placed between -1 and +1.\n",
    "2_ Windowing: For increasing the number of data and preparing them in order to feed them to model, windowing is crucial. Also, the length and shape of window and overlapping between consecutive windows are essential factors that cause variation in the accuracy of model. The length of window is an important factor that determines usecase of network for online and offline recognition.\n",
    "3_ Splitting data into train, validation and test set with the rate of 70, 15 and 15 percent. Also, test data has not be seen by model before.\n",
    "4_ Preparing target: The last layer of model in comprised of softmax layer. Therefor we need to transform labels into onehot form."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NRp8dPNL8ShN"
   },
   "source": [
    "#Data preprocessing with 16 channels and N samples\n",
    "def mode_rows(a):\n",
    "    a = np.ascontiguousarray(a)\n",
    "    void_dt = np.dtype((np.void, a.dtype.itemsize * np.prod(a.shape[1:])))\n",
    "    _,ids, count = np.unique(a.view(void_dt).ravel(), \\\n",
    "                                return_index=1,return_counts=1)\n",
    "    largest_count_id = ids[count.argmax()]\n",
    "    most_frequent_row = a[largest_count_id]\n",
    "    return most_frequent_row\n",
    "\n",
    "#import data from dataset file\n",
    "a = pd.read_pickle(r'/content/dualmyo_dataset.pkl')\n",
    "signal_list =a[0]\n",
    "label_list =a[1]\n",
    "\n",
    "signal = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "k = 0\n",
    "coef = np.ones([400,1])\n",
    "target = np.array([0])\n",
    "\n",
    "#windowing the signal\n",
    "for s in signal_list :\n",
    "  signal1 = np.hstack([abs(s[:,2:10]),abs(s[:,12:20])])\n",
    "  signal = np.vstack((signal,signal1))\n",
    "  target = np.vstack((target,label_list[k]*coef)) \n",
    "  k = k + 1\n",
    "\n",
    "print(signal.shape)    \n",
    "print(target.shape)   \n",
    "\n",
    "data = np.concatenate((signal,target),axis=1) \n",
    "\n",
    "seq_len = 200 \n",
    "stride = 200\n",
    "dataset=[]\n",
    "\n",
    "for i in range(0 , len(data)-seq_len , stride):\n",
    "    sample =  data[i:i+seq_len,:]\n",
    "    dataset.append(sample)\n",
    "\n",
    "#Splitting data into train and test sets\n",
    "#np.random.shuffle(dataset)\n",
    "train, test = sklearn.model_selection.train_test_split(dataset , train_size =int( np.round(0.85*len(dataset))) , test_size = int(np.round(0.15*len(dataset))), shuffle = False)\n",
    "\n",
    "\n",
    "x_train = np.zeros([len(train),seq_len,16,1])\n",
    "y_train = np.zeros([len(train),1])\n",
    "f=0\n",
    "for s  in train :\n",
    "    x_train[f,:,:,:] = s[:,0:16].reshape(-1,16,1)\n",
    "    y_train[f, 0] = mode_rows(s[seq_len-20:seq_len,16])\n",
    "    f = f+1\n",
    "#Normalizing input data and transforming output to onehot for train data\n",
    "x_tra = (x_train/128).astype('float32')\n",
    "y_tra = np_utils.to_categorical(y_train) \n",
    "\n",
    "\n",
    "\n",
    "x_test = np.zeros([len(test),seq_len,16,1])\n",
    "y_test = np.zeros([len(test),1])\n",
    "p=0\n",
    "for s  in test :\n",
    "    x_test[p,:,:,:] = s[:,0:16].reshape(-1,16,1)\n",
    "    y_test[p, 0] = mode_rows(s[seq_len-20:seq_len,16])\n",
    "    p = p+1\n",
    "#Normalizing input data and transforming output to onehot for test data\n",
    "x_tes = (x_test/128).astype('float32')\n",
    "y_tes = np_utils.to_categorical(y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "In this section model optimized in previous codes is created and tran with the data prepared in last section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "import keras\n",
    "from keras.initializers import RandomUniform\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "myInput = layers.Input(shape=(seq_len,16,1))\n",
    "conv1 = layers.Conv2D(131, (20,8), activation='relu',kernel_initializer=RandomUniform(), padding='same', strides=1)(myInput)\n",
    "conv2 = layers.Conv2D(28, (20,8), activation='relu',kernel_initializer=RandomUniform(), padding='same', strides=1)(conv1)\n",
    "flat = layers.Flatten()(conv2)\n",
    "dense1 = layers.Dense(111, activation='relu',kernel_initializer=RandomUniform())(flat)\n",
    "out_layer = layers.Dense(8, activation='softmax',kernel_initializer=RandomUniform())(dense1)\n",
    "\n",
    "model = Model(myInput, out_layer)\n",
    "\n",
    "#Compilie\n",
    "model.compile( optimizer=Adam (lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08 ) , loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "#model fit\n",
    "history = model.fit(x_tra , y_tra, epochs=30 , batch_size=64 ,shuffle=True , validation_split=0.15)\n",
    "print((history.history.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "In this section model is evaluated with the test set and confusion matrix for this data will be plotted. Also, loss and accuracy versus epoch charts from training steps are plotted.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6rdiu_WY8AO2"
   },
   "source": [
    "# Evaluate model with test data\n",
    "test_loss, test_acc = model.evaluate(x_tes, y_tes)\n",
    "print(test_acc)\n",
    "test_labels_p = model.predict(x_tes)\n",
    "y_pred = np.argmax(test_labels_p, axis=1)\n",
    "y_true = y_test\n",
    "y_true1 = y_true.astype('int64')\n",
    "y_true2 = y_true1.reshape(p,)\n",
    "\n",
    "\n",
    "# Get confusion matrix\n",
    "y_out = y_pred.tolist()\n",
    "t = y_true2.tolist()\n",
    "print(confusion_matrix(t , y_out,labels=[0, 1, 2, 3, 4, 5, 6, 7]))\n",
    "\n",
    "\n",
    "# plot accuracy and loss for train and validation set\n",
    "# summarize history for accuracy\n",
    "sns.set(style='darkgrid')\n",
    "plt.figure(figsize=(8, 4), dpi=100)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('/content/sample_data/accuracy.svg', format='svg', dpi=1200)\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "sns.set(style='darkgrid')\n",
    "plt.figure(figsize=(8, 4), dpi=100)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('/content/sample_data/loss.svg', format='svg', dpi=1200)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# END"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}